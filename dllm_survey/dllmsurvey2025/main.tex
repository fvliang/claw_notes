\documentclass{article}

% ---------------- ICLR-style layout ----------------
\usepackage{iclr2025_conference,times}

% ---------------- 调整页边距 ----------------
\usepackage{geometry}
\geometry{
  left=2cm,
  right=2cm,
  top=2cm,
  bottom=2cm,
  headheight=14pt,
  includeheadfoot
}
\fancyhfoffset{0pt}  % 让页眉页脚宽度与文本区一致
% ---------------- Common packages ----------------
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{xcolor}
\definecolor{pearDark}{RGB}{34,139,34}  
\definecolor{mygreen}{RGB}{34,139,34}
\definecolor{mylightblue}{RGB}{0,162,230}
\definecolor{deepyellow}{RGB}{255,215,0}
\definecolor{nvgreen}{RGB}{118, 185, 0}
\definecolor{codebg}{RGB}{245, 245, 245} 
\definecolor{keywordcolor}{RGB}{0, 0, 153} 
\definecolor{commentcolor}{RGB}{34, 139, 34} 
\definecolor{stringcolor}{RGB}{163, 21, 21}
\definecolor{numbercolor}{RGB}{128, 128, 128}
\definecolor{myblue}{HTML}{47B1E1}
\definecolor{mygreen}{HTML}{46D45A}
\definecolor{myorange}{HTML}{F3AA84}
\definecolor{usercolor}{HTML}{B41601}       
\definecolor{assistantcolor}{HTML}{76B900}  
\title{A New Language Modeling Paradigm: Diffusion Large Language Models}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

% ================= Title Page =================
\begin{titlepage}
\centering
\vspace*{3cm}

{\LARGE\bfseries A New Language Modeling Paradigm:\\[0.3em] Diffusion Large Language Models\par}

\vspace{2cm}

{\Large Fuliang Liu\par}
\vspace{0.5em}
{\large Nanjing University\par}
{\large School of Computer Science\par}
\vspace{0.3em}
{\large \texttt{fuliangliu@smail.nju.edu.cn}\par}

\vspace{2cm}

\begin{minipage}{0.85\textwidth}
\textbf{Abstract:} Autoregressive (AR) modeling dominates modern large language models (LLMs), but its left-to-right decoding is inherently sequential and restricts parallelism. Diffusion-based generative modeling provides an alternative: it generates by iteratively denoising from noise, offering bidirectional context and parallel token updates. This report surveys diffusion large language models (DLLMs) with an emphasis on the \emph{core insights} that make diffusion training scalable and discrete diffusion feasible. We first revisit score-based diffusion theory and explain why diffusion learns \emph{noise-conditional scores} rather than the raw data score, which turns score learning into a simple supervised regression. We then focus on discrete masked diffusion for language, highlighting SEDD and its key obstacle: timestep-dependent ratio estimation. We explain how RADD reveals an analytic timestep factorization that removes the need for a time-conditioned Transformer, simplifying the objective into an AR-like denoising cross-entropy. Next, we summarize SMDM's scaling-law perspective on masked diffusion LMs. Finally, we discuss systems and post-training conversion: why full-attention DLLMs cannot directly use KV cache, how Fast-dLLM enables KV cache and parallel decoding, and how recent \emph{AR$\rightarrow$Block-Diffusion} conversion (Block Diffusion, SDAR, Fast-dLLM v2) reuses pretrained AR capabilities while enabling blockwise parallel generation with efficient attention masks and AR-aligned prediction pathways.
\end{minipage}

\vfill
% {\large \today\par}
\end{titlepage}

% ================= Table of Contents =================
\tableofcontents
\newpage

% ================= Introduction =================
\section{Introduction}

Autoregressive (AR) language models factorize the joint probability of a token sequence into a product of next-token conditionals. This yields exact likelihood training and strong performance, but inference remains fundamentally sequential: tokens must be generated one-by-one, limiting decoding throughput.

Diffusion models represent a different paradigm. Instead of predicting tokens strictly left-to-right, diffusion models define a forward corruption process that gradually destroys structure, and learn a reverse process that iteratively denoises. When adapted to language, this yields diffusion large language models (DLLMs), which update many positions in parallel and can exploit bidirectional context during generation.

This report is organized around a central question: \emph{what are the key ideas that make diffusion language modeling both theoretically principled and practically scalable}? We therefore emphasize (i) the ``noise-conditioning'' insight that simplifies score learning in continuous diffusion, (ii) the discrete analogue of score learning via ratio/conditional estimation (SEDD), (iii) the major simplification brought by RADD (analytic timestep separation, removing time-conditioning), (iv) the scaling-law view of masked diffusion LMs (SMDM), and (v) system / post-training conversion techniques that make diffusion-style decoding deployable (Fast-dLLM, Block-Diffusion conversion such as SDAR and Fast-dLLM v2).

% ================= Diffusion Background =================
\section{Diffusion Models and Deep Generative Modeling}

This section presents the formula-centric backbone of diffusion as score-based modeling and explains a key insight often missed in ``formula-only'' expositions: \textbf{we do not directly train a network to predict the raw data score $\nabla_x \log p_{\text{data}}(x)$, but instead predict a \emph{noise-conditional} score.} This design is what makes diffusion training scalable.

\subsection{Score-Based Modeling and the Data Score}

Let $p_{\text{data}}(\mathbf{x})$ be the data distribution on $\mathbf{x}\in\mathbb{R}^d$. Score-based generative modeling aims to learn the score
\begin{equation}
\mathbf{s}^\ast(\mathbf{x}) \triangleq \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}).
\end{equation}
If $\mathbf{s}^\ast$ were known, one could sample from $p_{\text{data}}$ using Langevin dynamics / SDE-based sampling \cite{song2019sgm,song2021sde}.

\paragraph{Why not learn $\nabla_x\log p_{\text{data}}(x)$ directly?}
In high dimensions, $p_{\text{data}}$ often concentrates near a complicated low-dimensional manifold. The raw score can be ill-behaved (high curvature, unstable gradients) and, crucially, it is not paired with a clean supervised ``target'' for training. This is where diffusion introduces the key trick: \textbf{smooth the data distribution with noise, and learn the score of the smoothed distribution instead.}

\subsection{Noise-Conditional Score: The Scalability Insight}

Define a Gaussian corruption (a.k.a. diffusion ``forward noising''):
\begin{equation}
\mathbf{x}_\sigma = \mathbf{x}_0 + \sigma \boldsymbol{\epsilon},
\qquad \mathbf{x}_0\sim p_{\text{data}},\ \boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\mathbf{I}).
\end{equation}
Let $p_\sigma(\mathbf{x})$ be the marginal distribution of $\mathbf{x}_\sigma$. Diffusion trains a network $\mathbf{s}_\theta(\mathbf{x},\sigma)$ to approximate the \emph{noise-conditional score}
\begin{equation}
\mathbf{s}_\sigma(\mathbf{x}) \triangleq \nabla_{\mathbf{x}}\log p_\sigma(\mathbf{x}).
\end{equation}

\paragraph{Key benefit: supervised target becomes available.}
Under Gaussian corruption, the score matching objective admits a tractable form known as \emph{denoising score matching (DSM)} \cite{hyvarinen2005score,song2019sgm}:
\begin{equation}
\mathcal{L}_{\text{DSM}}(\theta)
=
\mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon}}
\left[
\left\|
\mathbf{s}_\theta(\mathbf{x}_0+\sigma\boldsymbol{\epsilon},\sigma)
+
\frac{\boldsymbol{\epsilon}}{\sigma}
\right\|_2^2
\right].
\label{eq:dsm}
\end{equation}
This reveals the core ``insight'':
\begin{itemize}
\item we \textbf{do not need} $\nabla_x \log p_{\text{data}}(x)$ as a label;
\item after adding noise, the optimal score estimator has a simple regression target $-\epsilon/\sigma$;
\item training reduces to standard supervised learning on synthetic noise.
\end{itemize}
This is precisely why diffusion training scales: the network learns a family of denoisers/scores across noise levels using easy-to-sample corruption.

\paragraph{Connection to ``predicting noise'' in DDPMs.}
DDPMs \cite{ho2020ddpm} further parameterize the reverse process by predicting $\epsilon$ (or an equivalent reparameterization), leading to the well-known objective
\begin{equation}
\mathcal{L}_{\text{DDPM}}(\theta)
=
\mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}}
\left[
\left\|
\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)
\right\|_2^2
\right],
\end{equation}
which is exactly the ``noise regression'' view implied by Eq.~\eqref{eq:dsm}. In short: \textbf{noise-conditioning transforms an intractable score-learning problem into a scalable denoising regression problem.}

\subsection{Sampling: From High Noise to Data}

Once $\mathbf{s}_\theta$ (or $\epsilon_\theta$) is trained, sampling proceeds from noise to data using annealed Langevin dynamics / reverse diffusion \cite{song2019sgm,song2021sde,ho2020ddpm}. This continuous backbone motivates discrete DLLMs: replace Gaussian noise with token masking, and replace continuous scores with discrete analogues (ratios / conditional token distributions).

% ================= Diffusion for Language =================
\section{Diffusion for Discrete Language Modeling}

For language, tokens are categorical so gradients w.r.t.\ tokens do not exist. Discrete diffusion therefore replaces ``add Gaussian noise'' with a token-space corruption process (typically masking), and replaces score estimation with learning discrete objects such as probability ratios (SEDD) or conditional clean-token distributions (RADD/SMDM/LLaDA).

Autoregressive models factorize
\begin{equation}
p_{\text{AR}}(x_{1:L})=\prod_{i=1}^L p(x_i\mid x_{<i}),
\end{equation}
while masked diffusion LMs model generation as iterative reconstruction from corrupted sequences, enabling parallel updates and bidirectional context \cite{lou2024sedd,ou2024radd,nie2024smdm,nie2025llada}.

% ================= SEDD =================
\section{SEDD: Score-Entropy Discrete Diffusion (Corrected)}

SEDD \cite{lou2024sedd} builds discrete diffusion around an \emph{absorbing} forward process: tokens are replaced by a special mask symbol $[\mathrm{M}]$ and remain masked thereafter. Let $x_0\in\mathcal{V}^L$ be a clean sequence. At time $t$, each position is independently masked with probability $1-\alpha_t$:
\begin{equation}
q(x_t \mid x_0)
=
\prod_{i=1}^L
\Big[
\alpha_t\,\mathbf{1}(x_t^i=x_0^i)
+
(1-\alpha_t)\,\mathbf{1}(x_t^i=[\mathrm{M}])
\Big].
\label{eq:sedd_forward}
\end{equation}

\subsection{What is the ``score'' in absorbing discrete diffusion? (Ratio view)}

The main obstacle in discrete diffusion is that $\nabla_x\log p(x)$ is undefined for categorical $x$. SEDD replaces ``gradient score'' with a \textbf{ratio-based object} called the \emph{concrete score}: compare the probability of two \emph{neighboring transitive states} that differ by changing one masked position into a concrete token. Concretely, if $x_t^i=[\mathrm{M}]$, define $\hat{x}_t^{(i\leftarrow v)}$ as the same sequence except position $i$ is set to $v\in\mathcal{V}$. The concrete score is a ratio of marginals at time $t$:
\begin{equation}
r_t(i,v \mid x_t)
\ \propto\
\frac{p_t(\hat{x}_t^{(i\leftarrow v)})}{p_t(x_t)}.
\label{eq:sedd_ratio}
\end{equation}
Intuitively, $r_t(i,v\mid x_t)$ measures how much more (or less) likely the sequence would be if the masked slot were concretized as $v$. This ratio plays the role of a discrete ``direction'' for denoising, analogous to how continuous scores guide movement from noise to data.

\subsection{Score Entropy: why SEDD can learn ratios without modeling $p_t$}

SEDD introduces \textbf{score entropy}, a loss that is the discrete analogue of score matching: instead of matching $\nabla_x\log p_\sigma(x)$, it matches the ratio structure needed by the reverse process \cite{lou2024sedd}. Operationally, SEDD trains a Transformer to output logits over vocabulary for each masked position, and uses these logits to parameterize the ratio estimator needed by the reverse kernel.

A practical implication (and also a pain point emphasized by later work) is:
\begin{itemize}
\item SEDD's ratio estimation is inherently \textbf{timestep-dependent}: the optimal ratio differs across $t$ because the marginal $p_t$ changes with masking severity.
\item Therefore, SEDD-style implementations typically require a \textbf{time-conditioned} network (explicit $t$ embeddings) to represent $r_t(\cdot)$ well.
\end{itemize}

This timestep dependence is exactly what RADD will simplify.

\subsection{SEDD training objective: Score Entropy and Denoising Score Entropy}
\label{subsec:sedd_loss}

SEDD's central contribution is to replace gradient-based score matching (undefined for categorical tokens) with a
\textbf{ratio-based} objective called \emph{score entropy}~\cite{lou2024sedd}. Let $p$ be a discrete distribution on states
(e.g., a corrupted token sequence state in a diffusion chain). SEDD introduces a \emph{score network}
$s_\theta(x)_y>0$ that assigns a positive ``score'' to transitioning from state $x$ to a different state $y$,
and defines the \textbf{score entropy loss}:
\begin{equation}
\mathcal{L}_{\mathrm{SE}}
=
\mathbb{E}_{x\sim p}
\left[
\sum_{y\neq x}
w_{xy}
\left(
s_\theta(x)_y
-
\frac{p(y)}{p(x)}\log s_\theta(x)_y
+
K\!\left(\frac{p(y)}{p(x)}\right)
\right)
\right],
\label{eq:sedd_score_entropy}
\end{equation}
where $w_{xy}\ge 0$ are user-chosen weights and
$K(a)=a(\log a-1)$ is a normalizing constant ensuring $\mathcal{L}_{\mathrm{SE}}\ge 0$~\cite{lou2024sedd}.
Intuitively, $\frac{p(y)}{p(x)}$ is exactly the \textbf{ratio signal} that we want the model to capture, and the optimal
solution satisfies $s_{\theta^\ast}(x)_y \propto \frac{p(y)}{p(x)}$ (up to the weighting scheme).

\paragraph{Why Eq.~\eqref{eq:sedd_score_entropy} is not directly scalable.}
The expression contains the unknown ratio $\frac{p(y)}{p(x)}$, where $p$ is the (time-dependent) diffusion marginal.
In large state spaces (language), directly estimating or summing over such ratios is intractable, which motivates a
denoising-style reformulation that only requires sampling from a tractable corruption kernel.

\paragraph{Denoising Score Entropy (scalable form).}
SEDD considers a perturbation view where the diffusion marginal $p$ is generated by perturbing a base density $p_0$
through a transition kernel $p(\cdot\mid x_0)$:
\begin{equation}
p(x)=\sum_{x_0} p(x\mid x_0)\,p_0(x_0).
\end{equation}
Then SEDD proves that $\mathcal{L}_{\mathrm{SE}}$ is equivalent (up to a $\theta$-independent constant) to the
\textbf{denoising score entropy} objective~\cite{lou2024sedd}:
\begin{equation}
\mathcal{L}_{\mathrm{DSE}}
=
\mathbb{E}_{x_0\sim p_0}
\mathbb{E}_{x\sim p(\cdot\mid x_0)}
\left[
\sum_{y\neq x}
w_{xy}
\left(
s_\theta(x)_y
-
\frac{p(y\mid x_0)}{p(x\mid x_0)}\log s_\theta(x)_y
\right)
\right].
\label{eq:sedd_denoising_score_entropy}
\end{equation}

\paragraph{The key scalability insight.}
Eq.~\eqref{eq:sedd_denoising_score_entropy} replaces the intractable ratio $\frac{p(y)}{p(x)}$
with a \emph{tractable conditional ratio} $\frac{p(y\mid x_0)}{p(x\mid x_0)}$ under the known perturbation kernel.
In diffusion language models, $p(\cdot\mid x_0)$ is exactly the forward corruption process (e.g., masking-based),
so these conditional probabilities can be computed analytically. As a result, Monte Carlo training only needs:
(i) sample $x_0\sim p_0$ (a clean data sample), (ii) sample $x$ from the forward corruption, and (iii) evaluate the
network once to obtain all $s_\theta(x)_y$.

\paragraph{Time-dependent score network.}
Since the diffusion marginal changes with timestep, SEDD typically parameterizes a time-conditioned score network
$s_\theta(x_t,t)$ and applies the denoising score entropy at each diffusion time using the corresponding forward kernel
$p_{t\mid 0}(\cdot\mid x_0)$~\cite{lou2024sedd}. This timestep dependence is precisely what RADD will later simplify by
analytically separating the $t$-dependence from the learned conditional prediction.


% ================= RADD =================
\section{RADD: Reparameterizing Absorbing Discrete Diffusion (The ``Simplification'' Insight)}

RADD \cite{ou2024radd} provides the key theoretical clarification: \textbf{in absorbing diffusion, the concrete score can be factorized into (i) a time-independent conditional probability of clean data and (ii) an analytic, time-dependent scalar.}

\subsection{Theorem-level message: isolate $t$ analytically}

RADD shows (informally summarized) that for masked positions, the ratio object needed by the reverse process can be expressed as
\begin{equation}
r_t(i,v \mid x_t)
=
c(t)\cdot
p(x_0^i=v \mid x_t),
\label{eq:radd_factorization}
\end{equation}
where $c(t)$ is an analytic scalar determined by the forward schedule, and the only hard part is the \textbf{time-independent} conditional distribution $p(x_0^i=v\mid x_t)$ \cite{ou2024radd}.

\paragraph{Why this matters.}
Eq.~\eqref{eq:radd_factorization} means we no longer need a Transformer that takes $t$ as an input just to represent the diffusion timestep effect. Instead:
\begin{itemize}
\item train a single (time-independent) denoiser $\pi_\theta(v\mid x_t)$;
\item incorporate timestep effects via a known scalar $c(t)$ outside the network.
\end{itemize}
This is the core \textbf{simplification}: \emph{RADD makes absorbing discrete diffusion look much closer to AR/MLM-style training.}

\subsection{Loss simplification: from time-conditioned ratio learning to AR-like denoising CE}

With timestep separated, the objective becomes essentially a masked denoising cross-entropy, potentially with an analytic weight induced by the schedule:
\begin{equation}
\mathcal{L}_{\text{RADD}}(\theta)
=
\mathbb{E}_{t}
\ \mathbb{E}_{x_0, x_t\sim q(\cdot\mid x_0,t)}
\left[
-\sum_{i: x_t^i=[\mathrm{M}]}
w(t)\,\log \pi_\theta(x_0^i \mid x_t)
\right],
\label{eq:radd_loss}
\end{equation}
where $w(t)$ is known (and can be derived from the forward process) \cite{ou2024radd}. Conceptually:
\begin{itemize}
\item \textbf{SEDD:} learn timestep-dependent ratios via a $t$-conditioned Transformer.
\item \textbf{RADD:} learn time-independent clean-conditionals; treat timestep as an analytic scalar.
\end{itemize}

This also explains RADD's practical advantages:
\begin{itemize}
\item simpler model interface (no $t$ embedding required for the core denoiser),
\item easier optimization and better cacheability during sampling (outputs can be reused when the noisy sample does not change),
\item clearer connection to any-order autoregressive models (AO-ARMs) \cite{ou2024radd}.
\end{itemize}

% ================= SMDM =================
\section{SMDM: Scaling up Masked Diffusion Models on Text}

SMDM \cite{nie2024smdm} addresses a gap in earlier discrete diffusion work: many papers validate only at limited scale, making it unclear whether masked diffusion LMs can scale like AR models. SMDM contributes two key insights: \textbf{(i) scaling-law evidence} and \textbf{(ii) simple mechanisms that significantly improve downstream performance.}

\subsection{Scaling laws: do masked diffusion LMs scale like AR?}

SMDM establishes one of the first scaling-law studies for masked diffusion models (MDMs) on text, showing that their loss scaling trend with compute/model/data can be comparable to autoregressive models, with a relatively small compute gap under controlled settings \cite{nie2024smdm}. This answers a fundamental concern: diffusion LMs are not inherently ``unscalable''; the main challenge is often \textbf{inference cost} (many denoising steps), not necessarily pretraining scalability.

\subsection{Why inference is still hard: NFE and step budget}

Even if training scales, generation typically requires multiple denoising iterations. Let $S$ be the number of denoising steps (number of function evaluations, NFE). Then latency roughly scales with $S$ forward passes. This makes system-level acceleration and/or reducing $S$ without quality collapse a first-class problem, motivating Fast-dLLM and block-diffusion conversion.

\subsection{Practical improvement: simple guidance for masked diffusion}

SMDM further highlights that diffusion-style generation quality can often be improved using simple guidance mechanisms, analogous in spirit to classifier-free guidance in continuous diffusion \cite{nie2024smdm}. At a high level, if the model can produce an ``unconditional'' prediction and a ``conditional'' prediction, one can interpolate logits to trade off diversity and fidelity:
\begin{equation}
\text{logits} \leftarrow (1+\gamma)\,\text{logits}_{\text{cond}} - \gamma\,\text{logits}_{\text{uncond}},
\end{equation}
where $\gamma$ controls guidance strength \cite{nie2024smdm}. This matters for language because masked diffusion often struggles with global coherence if steps are too few; guidance helps compensate.

% ================= Acceleration =================
\section{Fast-dLLM: Why KV Cache Breaks in Full-Attention DLLMs, and How to Fix It}


A critical deployment bottleneck for diffusion LMs is inference inefficiency. Compared to AR decoding, diffusion-style decoding often cannot directly benefit from standard KV cache, and also suffers when attempting to decode many tokens in parallel.

Fast-dLLM \cite{wu2025fastdllm} is important because it explicitly targets the \textbf{root systems issue}: enabling KV cache and parallel decoding for diffusion LLMs.

\subsection{Why ``standard KV cache'' does not work in full-attention diffusion}

In autoregressive decoding, KV cache works because:
\begin{itemize}
\item attention is causal (future tokens are never attended),
\item once a token is generated, its key/value contribution to later tokens is fixed,
\item we can reuse previously computed K/V for the prefix.
\end{itemize}

In many diffusion LMs (masked denoisers), attention is \textbf{bidirectional} within the whole sequence canvas. During sampling, the sequence content changes repeatedly due to masking/unmasking (and remasking). This breaks KV cache reuse for two reasons:
\begin{enumerate}
\item \textbf{Bidirectional dependence:} each position's representation can depend on \emph{all} other positions, including future slots. When any slot changes, it can affect many attention outputs.
\item \textbf{Remasking changes the ``input tokens'' repeatedly:} the set of masked vs.\ unmasked tokens changes across steps, so the hidden states (and thus K/V) are not stable across iterations.
\end{enumerate}
Therefore, naive reuse of K/V computed at an earlier denoising step is invalid and can cause quality collapse.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/kvcache.pdf}
    \caption{\textbf{Illustration of our Key-Value Cache for Block-Wise Decoding.} (a) During prefix-only caching, the KV cache is computed once for the prompt and reused across multiple decoding steps within each block. The cache is updated after completing a block to maintain consistency, with negligible overhead. 
    (b) DualCache extends this approach by caching both prefix and masked suffix tokens, further accelerating decoding. The high similarity of KV activations across steps allows effective reuse with minimal approximation error.
    }
    \label{fig: kv-cache}
\end{figure*}


\subsection{Fast-dLLM's answer: block-wise approximate cache + confidence-guided parallel decoding}

Fast-dLLM proposes a \textbf{block-wise approximate KV cache} tailored to diffusion LMs, enabling reuse with negligible quality drop \cite{wu2025fastdllm}, as shown in \ref{fig: kv-cache}. It also analyzes why naive parallel decoding degrades quality: unmasking many tokens assumes conditional independence and disrupts token dependencies. Fast-dLLM mitigates this using confidence-aware selection so that only sufficiently reliable predictions are accepted in parallel \cite{wu2025fastdllm}.

This section sets up the natural next step: instead of fighting full-sequence bidirectional attention, \textbf{design the model so that caching becomes structurally valid}---which is exactly what block-diffusion conversion (SDAR / Fast-dLLM v2 / Block Diffusion) does.

% ================= Block Diffusion Conversion =================
\section{AR $\rightarrow$ Block-Diffusion Conversion: Efficient Training Masks and AR-Aligned Prediction}
\label{sec:block_conversion}

A major recent direction is \emph{post-training conversion}: start from a strong pretrained AR LLM and adapt it into a \textbf{block-diffusion} generator, preserving AR capabilities while enabling parallel decoding \cite{arriola2025blockdiffusion,cheng2025sdar,wu2025fastdllmv2}. Compared to training diffusion LMs from scratch (e.g., large-scale MDM pretraining), conversion is far more data- and compute-efficient.

\subsection{Block diffusion: semi-autoregressive factorization}

Partition a length-$L$ sequence into $K$ contiguous blocks of size $D$:
\begin{equation}
x = (b_1,\dots,b_K),\qquad b_k \in \mathcal{V}^{D}, \qquad KD=L.
\end{equation}
Block diffusion keeps AR structure across blocks:
\begin{equation}
P_\theta(x) = \prod_{k=1}^{K} P_\theta(b_k \mid b_{<k}),
\label{eq:block_factorization}
\end{equation}
but implements each conditional $P_\theta(b_k\mid b_{<k})$ using \textbf{intra-block masked diffusion / iterative denoising}, allowing parallel token updates within a block \cite{arriola2025blockdiffusion,cheng2025sdar}.

\subsection{Attention Mask Design for Block-wise Diffusion Training}
\label{subsec:fastdllmv2_mask}

\subsection{Attention Mask Design}


\begin{figure*}[h]
\centering
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/train_attn_mask.pdf}
    \vspace{0.3em}
    
    (a) Training-time attention mask.
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/inference_attn_mask.pdf}
    \vspace{0.3em}
    
    (b) Inference-time attention mask.
\end{minipage}
\vspace{0.5em}
\caption{
Specialized attention mask design for diffusion language modeling. 
\textbf{(a)} During training, each input consists of a corrupted sequence \(x_t\) and corresponding targets \(x_0\), concatenated and processed in a single forward pass. 
The attention mask combines intra-block bidirectional attention (\textcolor{myblue}{Block Diagonal}), 
cross-block causal dependency from clean tokens to noised ones (\textcolor{mygreen}{Offset Block Causal}), 
and traditional left-to-right causality among clean tokens (\textcolor{myorange}{Block Causal}). 
\textbf{(b)} During inference, previously decoded blocks of \(x_0\) are reused via caching. 
Only the current noised block \(x_t\) is computed in each decoding step, which attends to cached prefixes (shaded) and updates its own block in a self-contained fashion.
}
\label{fig:attn_mask_comparison}
\end{figure*}


A core implementation trick in both Fast-dLLM v2 and SDAR is a \textbf{specialized $2L\times 2L$ attention mask} that enables
\emph{block-wise diffusion training on top of a pretrained AR LLM} in a \emph{single forward pass}~\cite{wu2025fastdllmv2}.
The key is to concatenate the \textbf{noised sequence} $x_t$ and the corresponding \textbf{clean sequence} $x_0$
into a length-$2L$ input:
\begin{equation}
\tilde{x} = [x_t;\ x_0]\in \mathcal{V}^{2L}.
\end{equation}
Here we slightly abuse notation as in the appendix: $x^b$ denotes the set of tokens in block $b$ (block size $D$),
and the training target is the block-conditional distribution
\begin{equation}
p_\theta(x^b \mid x_t^b,\ x^{<b}),
\end{equation}
where $x_t^b$ is the noised version of the current block and $x^{<b}$ are all \emph{clean} tokens in previous blocks.

\paragraph{Block-structured $2L\times 2L$ mask.}
Fast-dLLM v2 constructs a full mask $\mathcal{M}_{\text{full}}\in\{0,1\}^{2L\times 2L}$ with a \textbf{2-by-2 block form}, which is shown as \ref{fig:attn_mask_comparison}(a):
\begin{equation}
\mathcal{M}_{\text{full}}
=
\begin{bmatrix}
\mathcal{M}_{\mathrm{BD}} & \mathcal{M}_{\mathrm{OBC}} \\
0 & \mathcal{M}_{\mathrm{BC}}
\end{bmatrix}.
\label{eq:fastdllmv2_mfull}
\end{equation}
This design explicitly separates (i) \textbf{within-block bidirectional refinement} in the noised part, and
(ii) \textbf{block-level causal semantics} carried by the clean part, while preventing information leakage
from $x_t$ back into $x_0$.

\paragraph{(1) $\mathcal{M}_{\mathrm{BD}}$: Block-diagonal bidirectional attention (inside $x_t$).}
Within the noised half $x_t$, tokens only attend bidirectionally \emph{within the same block}, enabling parallel refinement:
\begin{equation}
[\mathcal{M}_{\mathrm{BD}}]_{ij} =
\begin{cases}
1, & \text{if } i,j \text{ belong to the same block},\\
0, & \text{otherwise}.
\end{cases}
\label{eq:mbd}
\end{equation}
So the model performs diffusion-style denoising \emph{locally within each block} rather than globally across the whole sequence.

\paragraph{(2) $\mathcal{M}_{\mathrm{OBC}}$: Offset block-causal attention (from $x_t$ to $x_0$ prefixes).}
Each noised token is allowed to attend to \emph{clean tokens in previous blocks} from the $x_0$ half:
\begin{equation}
[\mathcal{M}_{\mathrm{OBC}}]_{ij} =
\begin{cases}
1, & \text{if } j \text{ is in a block before } i,\\
0, & \text{otherwise}.
\end{cases}
\label{eq:mobc}
\end{equation}
This is the crucial ``\textbf{conditioning channel}'' that realizes $p_\theta(x^b\mid x_t^b, x^{<b})$:
the current noised block can use the \emph{clean prefix blocks} as fixed causal context, matching AR semantics across blocks.

\paragraph{(3) $0$: No leakage from $x_t$ back to $x_0$.}
The lower-left block is set to zero so tokens in the clean half $x_0$ \emph{never attend to} the noised half $x_t$.
This avoids contaminating the clean-stream representations with time-dependent noise patterns, which is important for
stability and for preserving AR-like behavior.

\paragraph{(4) $\mathcal{M}_{\mathrm{BC}}$: Block-causal attention (inside $x_0$).}
Inside the clean half $x_0$, Fast-dLLM v2 uses a \textbf{block-causal} mask: a token can attend to all tokens in the
same block and all previous blocks:
\begin{equation}
[\mathcal{M}_{\mathrm{BC}}]_{ij} =
\begin{cases}
1, & \text{if } j \text{ is in the same or an earlier block as } i,\\
0, & \text{otherwise}.
\end{cases}
\label{eq:mbc}
\end{equation}
This enforces left-to-right generation \emph{at the block level}, while allowing full interaction \emph{within a block}
(which matches the block-diffusion decoding style used at inference).

\paragraph{Why the $2L\times 2L$ mask matters (the correct takeaway).}
This mask is not merely ``two views''; it is a \textbf{structural factorization} that makes block-wise diffusion training
efficient and AR-consistent:
\begin{itemize}
\item It trains the desired conditional $p_\theta(x^b\mid x_t^b, x^{<b})$ by letting $x_t$ attend to clean prefix blocks (via $\mathcal{M}_{\mathrm{OBC}}$),
while keeping refinement local within blocks (via $\mathcal{M}_{\mathrm{BD}}$).
\item It preserves AR inductive bias by ensuring the clean stream is block-causal and never sees the noised stream (bottom-left zero block).
\item It sets up inference-time caching naturally: previously decoded clean blocks behave as a frozen prefix context, and only the current noised block needs iterative refinement~\cite{wu2025fastdllmv2}.
\end{itemize}



\subsection{SDAR: lightweight AR-to-block-diffusion conversion}

SDAR \cite{cheng2025sdar} performs a paradigm conversion:
\begin{enumerate}
\item start from pretrained AR $\theta_{\text{AR}}$,
\item continue training with a conditional block-denoising objective,
\item decode autoregressively across blocks, but denoise in parallel within each block.
\end{enumerate}
The key advantage is conceptual and practical: AR provides strong global coherence and training efficiency; diffusion provides intra-block parallelism and bidirectional refinement \cite{cheng2025sdar}.

\subsection{Fast-dLLM v2: SDAR-like block diffusion, plus two crucial ``engineering'' differences}

Fast-dLLM v2 \cite{wu2025fastdllmv2} is extremely close in paradigm to SDAR (both are AR$\rightarrow$block diffusion conversion), but it emphasizes two differences you highlighted:

\paragraph{(i) Shifted prediction (AR-aligned prediction pathway).}
Masked-token prediction normally uses the same position's hidden state. But AR models are trained to predict $x_i$ from position $i\!-\!1$ hidden state (NTP geometry). Fast-dLLM v2 therefore uses \textbf{shifted prediction}: predict the token at $i$ using the hidden state at $i-1$ when $x_i$ is masked. This preserves AR representations and stabilizes conversion \cite{wu2025fastdllmv2}.

\paragraph{(ii) Complementary masking (stronger and more balanced supervision).}
Fast-dLLM v2 uses complementary masks so every token is trained both as context and as target (in expectation), improving data efficiency \cite{wu2025fastdllmv2}.

\paragraph{(iii) Hierarchical caching for fast inference.}
Beyond the conversion objective, Fast-dLLM v2 designs block-level and sub-block caches to support high-throughput inference, achieving strong speedups while maintaining accuracy \cite{wu2025fastdllmv2}.

\subsection{Unifying view: what SDAR and Fast-dLLM v2 are really doing}

Both SDAR and Fast-dLLM v2 can be seen as optimizing a conditional denoising objective compatible with a pretrained AR backbone:
\begin{equation}
\max_\theta\;
\mathbb{E}_{k,t}
\ \mathbb{E}_{\text{corruption}}
\left[
\log P_\theta\!\left(b_k \mid \mathrm{corrupt}_t(b_k),\, b_{<k}\right)
\right],
\end{equation}
while enforcing \textbf{AR-consistent structure across blocks} and \textbf{parallel refinement within blocks}. Their primary difference is how aggressively they preserve AR geometry (shifted prediction) and how they structure training/inference engineering (complementary masks, hierarchical cache) \cite{cheng2025sdar,wu2025fastdllmv2}.

% ================= Conclusion =================
\section{Conclusion}

Diffusion language modeling becomes practical only when we understand (and exploit) the right abstractions. In continuous diffusion, the essential scalability insight is to learn \emph{noise-conditional} scores rather than the raw data score, turning score learning into supervised noise regression. In discrete language diffusion, SEDD shows how to replace gradients with ratio-based ``scores'', while RADD provides a key simplification: the timestep dependence can be separated analytically, removing the need for a time-conditioned Transformer and reducing training to an AR-like denoising cross-entropy. At scale, SMDM provides evidence that masked diffusion LMs can follow competitive scaling trends, shifting the main challenge to inference efficiency. Finally, systems and conversion techniques close the deployment gap: Fast-dLLM explains why full-attention DLLMs cannot directly use KV cache and proposes cache- and confidence-aware acceleration, while block-diffusion conversion (Block Diffusion, SDAR, Fast-dLLM v2) offers a compelling ``best of both worlds'': preserve pretrained AR capabilities and coherence across blocks, but enable parallel denoising within blocks through efficient attention masks and AR-aligned prediction pathways.

\bibliographystyle{iclr2025_conference}
\bibliography{iclr2025_conference}

\end{document}
